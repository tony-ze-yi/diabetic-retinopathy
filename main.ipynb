{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models, datasets\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import collections\n",
    "import sys\n",
    "from mpemu import mpt_emu\n",
    "from mpemu import qutils\n",
    "import timm\n",
    "sys.path.insert(0, \"./ViT-pytorch\")\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "import numpy as np\n",
    "# Define paths and hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 53\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "QUANTIZE = False\n",
    "TEST_MODEL = True\n",
    "\n",
    "# quantization\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# filter_module_types = [torch.nn.Conv2d, torch.nn.Linear] # Only quantizing convolution and linear modules\n",
    "# exempt_modules = [\"conv1\",\"fc\"]\n",
    "\n",
    "print()\n",
    "# define the EfficientNet models to use\n",
    "# b0 max: 64\n",
    "# b3 max: 32?\n",
    "efficientnets = {\n",
    "    # \"b0\": EfficientNet.from_pretrained(\"efficientnet-b0\", num_classes=5),\n",
    "    # \"b1\": EfficientNet.from_pretrained(\"efficientnet-b1\", num_classes=5),\n",
    "    # \"b2\": EfficientNet.from_pretrained(\"efficientnet-b2\", num_classes=5),\n",
    "    # \"b3\": EfficientNet.from_pretrained(\"efficientnet-b3\", num_classes=5),\n",
    "    # \"b4\": EfficientNet.from_pretrained(\"efficientnet-b4\", num_classes=5),\n",
    "    # \"b5\": EfficientNet.from_pretrained(\"efficientnet-b5\", num_classes=5),\n",
    "    # \"b6\": EfficientNet.from_pretrained(\"efficientnet-b6\", num_classes=5),\n",
    "    # \"b7\": EfficientNet.from_pretrained(\"efficientnet-b7\", num_classes=5),\n",
    "}\n",
    "\n",
    "efficientnet_sizes = {\n",
    "    \"b0\": 224,\n",
    "    \"b1\": 240,\n",
    "    \"b2\": 260,\n",
    "    \"b3\": 300,\n",
    "    \"b4\": 380,\n",
    "    \"b5\": 456,\n",
    "    \"b6\": 528,\n",
    "    \"b7\": 600,\n",
    "}\n",
    "# model_name = \"vit_base_r50_s16_384\"\n",
    "model_name = \"R50-ViT-B_16\"\n",
    "# resnext = timm.create_model(model_name, pretrained=True, num_classes=5)\n",
    "vit = VisionTransformer(CONFIGS[model_name], 384, zero_head=True, num_classes=5)\n",
    "model_ckpt = torch.load(\"r50/checkpoint_8_noncont.pt\", map_location=\"cpu\")\n",
    "vit.load_state_dict(model_ckpt[\"model_state_dict\"] if \"model_state_dict\" in model_ckpt else model_ckpt, strict=False)\n",
    "# vit.load_from(np.load(\"R50-ViT-B_16.npz\"))\n",
    "vit.to(DEVICE)\n",
    "models = [vit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convit_base',\n",
       " 'convit_small',\n",
       " 'convit_tiny',\n",
       " 'crossvit_9_240',\n",
       " 'crossvit_9_dagger_240',\n",
       " 'crossvit_15_240',\n",
       " 'crossvit_15_dagger_240',\n",
       " 'crossvit_15_dagger_408',\n",
       " 'crossvit_18_240',\n",
       " 'crossvit_18_dagger_240',\n",
       " 'crossvit_18_dagger_408',\n",
       " 'crossvit_base_240',\n",
       " 'crossvit_small_240',\n",
       " 'crossvit_tiny_240',\n",
       " 'gcvit_base',\n",
       " 'gcvit_small',\n",
       " 'gcvit_tiny',\n",
       " 'gcvit_xtiny',\n",
       " 'gcvit_xxtiny',\n",
       " 'levit_128',\n",
       " 'levit_128s',\n",
       " 'levit_192',\n",
       " 'levit_256',\n",
       " 'levit_256d',\n",
       " 'levit_384',\n",
       " 'maxvit_base_224',\n",
       " 'maxvit_large_224',\n",
       " 'maxvit_nano_rw_256',\n",
       " 'maxvit_pico_rw_256',\n",
       " 'maxvit_rmlp_nano_rw_256',\n",
       " 'maxvit_rmlp_pico_rw_256',\n",
       " 'maxvit_rmlp_small_rw_224',\n",
       " 'maxvit_rmlp_small_rw_256',\n",
       " 'maxvit_rmlp_tiny_rw_256',\n",
       " 'maxvit_small_224',\n",
       " 'maxvit_tiny_224',\n",
       " 'maxvit_tiny_pm_256',\n",
       " 'maxvit_tiny_rw_224',\n",
       " 'maxvit_tiny_rw_256',\n",
       " 'maxvit_xlarge_224',\n",
       " 'maxxvit_rmlp_nano_rw_256',\n",
       " 'maxxvit_rmlp_small_rw_256',\n",
       " 'maxxvit_rmlp_tiny_rw_256',\n",
       " 'mobilevit_s',\n",
       " 'mobilevit_xs',\n",
       " 'mobilevit_xxs',\n",
       " 'mobilevitv2_050',\n",
       " 'mobilevitv2_075',\n",
       " 'mobilevitv2_100',\n",
       " 'mobilevitv2_125',\n",
       " 'mobilevitv2_150',\n",
       " 'mobilevitv2_150_384_in22ft1k',\n",
       " 'mobilevitv2_150_in22ft1k',\n",
       " 'mobilevitv2_175',\n",
       " 'mobilevitv2_175_384_in22ft1k',\n",
       " 'mobilevitv2_175_in22ft1k',\n",
       " 'mobilevitv2_200',\n",
       " 'mobilevitv2_200_384_in22ft1k',\n",
       " 'mobilevitv2_200_in22ft1k',\n",
       " 'mvitv2_base',\n",
       " 'mvitv2_large',\n",
       " 'mvitv2_small',\n",
       " 'mvitv2_small_cls',\n",
       " 'mvitv2_tiny',\n",
       " 'semobilevit_s',\n",
       " 'vit_base_patch8_224',\n",
       " 'vit_base_patch8_224_dino',\n",
       " 'vit_base_patch8_224_in21k',\n",
       " 'vit_base_patch16_18x2_224',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_224_dino',\n",
       " 'vit_base_patch16_224_in21k',\n",
       " 'vit_base_patch16_224_miil',\n",
       " 'vit_base_patch16_224_miil_in21k',\n",
       " 'vit_base_patch16_224_sam',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch16_plus_240',\n",
       " 'vit_base_patch16_rpn_224',\n",
       " 'vit_base_patch32_224',\n",
       " 'vit_base_patch32_224_clip_laion2b',\n",
       " 'vit_base_patch32_224_in21k',\n",
       " 'vit_base_patch32_224_sam',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_patch32_plus_256',\n",
       " 'vit_base_r26_s32_224',\n",
       " 'vit_base_r50_s16_224',\n",
       " 'vit_base_r50_s16_224_in21k',\n",
       " 'vit_base_r50_s16_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50_224_in21k',\n",
       " 'vit_base_resnet50_384',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_giant_patch14_224',\n",
       " 'vit_giant_patch14_224_clip_laion2b',\n",
       " 'vit_gigantic_patch14_224',\n",
       " 'vit_huge_patch14_224',\n",
       " 'vit_huge_patch14_224_clip_laion2b',\n",
       " 'vit_huge_patch14_224_in21k',\n",
       " 'vit_large_patch14_224',\n",
       " 'vit_large_patch14_224_clip_laion2b',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_224_in21k',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_224',\n",
       " 'vit_large_patch32_224_in21k',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_large_r50_s32_224',\n",
       " 'vit_large_r50_s32_224_in21k',\n",
       " 'vit_large_r50_s32_384',\n",
       " 'vit_relpos_base_patch16_224',\n",
       " 'vit_relpos_base_patch16_cls_224',\n",
       " 'vit_relpos_base_patch16_clsgap_224',\n",
       " 'vit_relpos_base_patch16_plus_240',\n",
       " 'vit_relpos_base_patch16_rpn_224',\n",
       " 'vit_relpos_base_patch32_plus_rpn_256',\n",
       " 'vit_relpos_medium_patch16_224',\n",
       " 'vit_relpos_medium_patch16_cls_224',\n",
       " 'vit_relpos_medium_patch16_rpn_224',\n",
       " 'vit_relpos_small_patch16_224',\n",
       " 'vit_relpos_small_patch16_rpn_224',\n",
       " 'vit_small_patch8_224_dino',\n",
       " 'vit_small_patch16_18x2_224',\n",
       " 'vit_small_patch16_36x1_224',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_patch16_224_dino',\n",
       " 'vit_small_patch16_224_in21k',\n",
       " 'vit_small_patch16_384',\n",
       " 'vit_small_patch32_224',\n",
       " 'vit_small_patch32_224_in21k',\n",
       " 'vit_small_patch32_384',\n",
       " 'vit_small_r26_s32_224',\n",
       " 'vit_small_r26_s32_224_in21k',\n",
       " 'vit_small_r26_s32_384',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s16_224',\n",
       " 'vit_srelpos_medium_patch16_224',\n",
       " 'vit_srelpos_small_patch16_224',\n",
       " 'vit_tiny_patch16_224',\n",
       " 'vit_tiny_patch16_224_in21k',\n",
       " 'vit_tiny_patch16_384',\n",
       " 'vit_tiny_r_s16_p8_224',\n",
       " 'vit_tiny_r_s16_p8_224_in21k',\n",
       " 'vit_tiny_r_s16_p8_384']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models(\"*vit*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 53/55 [01:16<00:02,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from lion_pytorch import Lion\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train():\n",
    "    # train the models and evaluate them on the validation set\n",
    "    # for model_name, model in efficientnets.items():\n",
    "    for model in models:\n",
    "        # define the transform for data augmentation and resizing\n",
    "        # image_size = efficientnet_sizes[model_name]\n",
    "        image_size = 384\n",
    "        # Define data augmentations and transformations\n",
    "        train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        val_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                # transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create train and validation datasets\n",
    "        train_dataset = datasets.ImageFolder(\"/mnt/onetb/diabetic-retinopathy/FasterTransformer/examples/pytorch/vit/ViT-quantization/data_512_split/train\", transform=train_transforms)\n",
    "        val_dataset = datasets.ImageFolder(\"/mnt/onetb/diabetic-retinopathy/FasterTransformer/examples/pytorch/vit/ViT-quantization/data_512_split/val\", transform=val_transforms)\n",
    "        print(train_dataset.classes)\n",
    "\n",
    "#         train_idx, val_idx = train_test_split(\n",
    "#             list(range(len(dataset.targets))), test_size=0.2, stratify=dataset.targets\n",
    "#         )\n",
    "#         train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "#         val_dataset = torch.utils.data.Subset(dataset_val, val_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        # Define model\n",
    "#         model.to(DEVICE)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # optimizer = Lion(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "        #         optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=5e-4,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=0)\n",
    "\n",
    "        list_exempt_layers = [\"conv1\", \"fc\"]\n",
    "        #         if \"resnet\" in args.arch or \"resnext\" in args.arch:\n",
    "        #             list_exempt_layers = [\"conv1\",\"fc\"]\n",
    "        #         elif args.arch == \"vgg19_bn\":\n",
    "        #             list_exempt_layers = [\"features.0\", \"classifier.6\"]\n",
    "        #         elif args.arch == \"inception_v3\":\n",
    "        #             list_exempt_layers = [\"Conv2d_1a_3x3.conv\", \"fc\"]\n",
    "\n",
    "        # quantization\n",
    "        if QUANTIZE:\n",
    "            model, emulator = mpt_emu.quantize_model(\n",
    "                model,\n",
    "                optimizer=optimizer,\n",
    "                dtype=\"e4m3\",\n",
    "                device=DEVICE,\n",
    "                verbose=True,\n",
    "                list_exempt_layers=list_exempt_layers,\n",
    "                list_layers_output_fused=[],\n",
    "            )\n",
    "            emulator.set_default_inference_qconfig()\n",
    "        #             filter_module_types = [torch.nn.Conv2d, torch.nn.Linear] # Only quantizing convolution and linear modules\n",
    "        #             exempt_modules = [\"conv1\",\"fc\"]\n",
    "        #             is_training = True\n",
    "        #             wt_qconfig = qutils.TensorQuantConfig(\"e4m3\", \"rne\")#, \"per-channel\")\n",
    "        #             iact_qconfig   = qutils.TensorQuantConfig(\"e4m3\", \"rne\")#, \"per-tensor\")\n",
    "        #             emb_qconfig    = qutils.TensorQuantConfig(\"e4m3\", \"rne\")#, \"per-channel\")\n",
    "        #             qconfig = qutils.ModuleQuantConfig(wt_qconfig=wt_qconfig, iact_qconfig=iact_qconfig)\n",
    "        #             model_qconfig_dict  = qutils.get_or_update_model_quant_config_dict(model, filter_module_types, qconfig,\n",
    "        #                                                                         exempt_modules=exempt_modules)\n",
    "        #             print(\"Model quantization configuration\")\n",
    "        #             for layer,qconfig in model_qconfig_dict.items():\n",
    "        #                 print(layer, qconfig)\n",
    "        #             print()\n",
    "\n",
    "        #             qutils.reset_quantization_setup(model, model_qconfig_dict)\n",
    "        #             qhooks = qutils.add_quantization_hooks(model, model_qconfig_dict, is_training=is_training)\n",
    "        # test the model\n",
    "        if TEST_MODEL:\n",
    "            if QUANTIZE:\n",
    "                eval_model = emulator.fuse_bnlayers_and_quantize_model(model)\n",
    "            val_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(val_loader):\n",
    "                    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                    if QUANTIZE:\n",
    "                        outputs = eval_model(images)\n",
    "                    else:\n",
    "                        outputs, _ = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    val_acc += torch.sum(predictions == labels.data)\n",
    "            val_loss /= len(val_dataset)\n",
    "            val_acc /= len(val_dataset)\n",
    "            print(torch.cuda.memory_stats(DEVICE))\n",
    "            print(f\"Val   loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "            return\n",
    "\n",
    "        # Train and validate the model\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            start = time.time()\n",
    "            start_total = time.time()\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "            # Train the model\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            for images, labels in tqdm(train_loader):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs, _ = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_loss += loss.item() * images.size(0)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                train_acc += torch.sum(predictions == labels.data)\n",
    "            train_loss /= len(train_dataset)\n",
    "            train_acc /= len(train_dataset)\n",
    "            print(f\"Train loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "            print(f\"Train time: {time.time() - start}\")\n",
    "            print(torch.cuda.memory_stats(DEVICE))\n",
    "\n",
    "            start = time.time()\n",
    "            # Validate the model\n",
    "            if QUANTIZE:\n",
    "                eval_model = emulator.fuse_bnlayers_and_quantize_model(model)\n",
    "            val_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(val_loader):\n",
    "                    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                    if QUANTIZE:\n",
    "                        outputs = eval_model(images)\n",
    "                    else:\n",
    "                        outputs, _ = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    val_acc += torch.sum(predictions == labels.data)\n",
    "            val_loss /= len(val_dataset)\n",
    "            val_acc /= len(val_dataset)\n",
    "            print(f\"Val   loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            checkpoint_path = f\"checkpoint_{epoch+1}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_acc\": val_acc,\n",
    "                },\n",
    "                checkpoint_path,\n",
    "            )\n",
    "            total_end = time.time() - start_total\n",
    "\n",
    "            # Save loss and accuracy values to file\n",
    "            with open(\"loss_acc.txt\", \"a\") as file:\n",
    "                file.write(\n",
    "                    f\"{model_name}, {train_loss:.4f}, {train_acc:.4f}, {val_loss:.4f}, {val_acc:.4f}, {epoch}, {BATCH_SIZE}, {total_end}\\n\"\n",
    "                )\n",
    "\n",
    "            print(f\"Val and misc time: {time.time() - start}\")\n",
    "            print(f\"Total time: {total_end}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
