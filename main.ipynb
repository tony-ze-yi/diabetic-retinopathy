{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models, datasets\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Define paths and hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define the EfficientNet models to use\n",
    "# b0 max: 64\n",
    "# b3 max: 32?\n",
    "efficientnets = {\n",
    "    \"b0\": EfficientNet.from_pretrained(\"efficientnet-b0\", num_classes=5),\n",
    "    # \"b1\": EfficientNet.from_pretrained(\"efficientnet-b1\", num_classes=5),\n",
    "    # \"b2\": EfficientNet.from_pretrained(\"efficientnet-b2\", num_classes=5),\n",
    "    # \"b3\": EfficientNet.from_pretrained(\"efficientnet-b3\", num_classes=5),\n",
    "    # \"b4\": EfficientNet.from_pretrained(\"efficientnet-b4\", num_classes=5),\n",
    "    # \"b5\": EfficientNet.from_pretrained(\"efficientnet-b5\", num_classes=5),\n",
    "    # \"b6\": EfficientNet.from_pretrained(\"efficientnet-b6\", num_classes=5),\n",
    "    # \"b7\": EfficientNet.from_pretrained(\"efficientnet-b7\", num_classes=5),\n",
    "}\n",
    "\n",
    "efficientnet_sizes = {\n",
    "    \"b0\": 224,\n",
    "    \"b1\": 240,\n",
    "    \"b2\": 260,\n",
    "    \"b3\": 300,\n",
    "    \"b4\": 380,\n",
    "    \"b5\": 456,\n",
    "    \"b6\": 528,\n",
    "    \"b7\": 600,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4']\n",
      "Epoch 1/10\n",
      "Train loss: 0.7704, Acc: 0.7460\n",
      "Train time: 306.99389696121216\n",
      "Val   loss: 0.9119, Acc: 0.7687\n",
      "Val and misc time: 82.02202558517456\n",
      "Epoch 2/10\n",
      "Train loss: 0.6399, Acc: 0.7859\n",
      "Train time: 297.63864612579346\n",
      "Val   loss: 0.6494, Acc: 0.7800\n",
      "Val and misc time: 83.00439262390137\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 119\u001B[0m\n\u001B[0;32m    115\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVal and misc time: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mstart\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 119\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 67\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     65\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     66\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 67\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     69\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mG:\\GitHub\\diabetic-retinopathy\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mG:\\GitHub\\diabetic-retinopathy\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mG:\\GitHub\\diabetic-retinopathy\\venv\\lib\\site-packages\\lion_pytorch\\lion_pytorch.py:78\u001B[0m, in \u001B[0;36mLion.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m     74\u001B[0m             state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexp_avg\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(p)\n\u001B[0;32m     76\u001B[0m         exp_avg \u001B[38;5;241m=\u001B[39m state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexp_avg\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 78\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_fn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[43m            \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     80\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[43m            \u001B[49m\u001B[43mexp_avg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbeta2\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mG:\\GitHub\\diabetic-retinopathy\\venv\\lib\\site-packages\\lion_pytorch\\lion_pytorch.py:16\u001B[0m, in \u001B[0;36mupdate_fn\u001B[1;34m(p, grad, exp_avg, lr, wd, beta1, beta2)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_fn\u001B[39m(p, grad, exp_avg, lr, wd, beta1, beta2):\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# stepweight decay\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m     \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mwd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# weight update\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     update \u001B[38;5;241m=\u001B[39m exp_avg\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd(grad, alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\u001B[38;5;241m.\u001B[39msign_()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "def train():\n",
    "    # train the models and evaluate them on the validation set\n",
    "    for model_name, model in efficientnets.items():\n",
    "        # define the transform for data augmentation and resizing\n",
    "        image_size = efficientnet_sizes[model_name]\n",
    "        # Define data augmentations and transformations\n",
    "        train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        val_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                # transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create train and validation datasets\n",
    "        dataset = datasets.ImageFolder(\"data_sorted\", transform=train_transforms)\n",
    "        dataset_val = datasets.ImageFolder(\"data_sorted\", transform=val_transforms)\n",
    "        print(dataset.classes)\n",
    "\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            list(range(len(dataset.targets))), test_size=0.2, stratify=dataset.targets\n",
    "        )\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_dataset = torch.utils.data.Subset(dataset_val, val_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Define model\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "        # Train and validate the model\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            start = time.time()\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "            # Train the model\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_loss += loss.item() * images.size(0)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                train_acc += torch.sum(predictions == labels.data)\n",
    "            train_loss /= len(train_dataset)\n",
    "            train_acc /= len(train_dataset)\n",
    "            print(f\"Train loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "            print(f\"Train time: {time.time() - start}\")\n",
    "\n",
    "            start = time.time()\n",
    "            # Validate the model\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    val_acc += torch.sum(predictions == labels.data)\n",
    "            val_loss /= len(val_dataset)\n",
    "            val_acc /= len(val_dataset)\n",
    "            print(f\"Val   loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            checkpoint_path = f\"checkpoint_{epoch+1}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_acc\": val_acc,\n",
    "                },\n",
    "                checkpoint_path,\n",
    "            )\n",
    "\n",
    "            # Save loss and accuracy values to file\n",
    "            with open(\"loss_acc.txt\", \"a\") as file:\n",
    "                file.write(\n",
    "                    f\"{model_name}, {train_loss:.4f}, {train_acc:.4f}, {val_loss:.4f}, {val_acc:.4f}, {epoch}, {BATCH_SIZE}\\n\"\n",
    "                )\n",
    "\n",
    "            print(f\"Val and misc time: {time.time() - start}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
